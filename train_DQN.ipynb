{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert image to a 80*80 gray image\n",
    "def preprocess(observation):\n",
    "    img = cv2.resize(observation, (80, 80))\n",
    "    observation = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(1,80,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.map_size = (64, 10, 10)\n",
    "        self.fc1 = nn.Linear(self.map_size[0]*self.map_size[1]*self.map_size[2], 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #forward procedure\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = self.fc2\n",
    "        return x\n",
    "\n",
    "class DQNBrain(object):\n",
    "    empty_frame = np.zeros((80, 80), dtype=np.float32)\n",
    "    empty_state = np.stack((empty_frame, empty_frame, empty_frame, empty_frame), axis=0)\n",
    "    def __init__(self, cfg):\n",
    "        #init all hyperparameters\n",
    "        self.is_training = cfg.is_training \n",
    "        self.is_cuda = cfg.is_cuda\n",
    "        self.lr = cfg.lr\n",
    "        self.mem_size = cfg.mem_size\n",
    "        self.actions = cfg.actions\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.time_step = 0\n",
    "        self.gamma = cfg.gamma\n",
    "        self.init_e = cfg.init_e\n",
    "        self.final_e = cfg.final_e \n",
    "\n",
    "        #init model state\n",
    "        self.currt_state = self.empty_state\n",
    "\n",
    "        #init Q network\n",
    "        self.model = Net()\n",
    "        if self.is_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        #init replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        #init training\n",
    "        self.optimizer = torch.optim.RMSprop((self.model).parameters(), lr=self.lr)\n",
    "        self.ceriterion = nn.MSELoss()\n",
    "\n",
    "    def store_transition(self, o_next, action, reward, terminal):\n",
    "        \"\"\"\n",
    "        #o_next \n",
    "        param: next observation\n",
    "        type: numpy array, [1,80,80]\n",
    "        #action\n",
    "        param: bird action\n",
    "        type: numpy array, [2]\n",
    "        #reward\n",
    "        param: agent reward\n",
    "        #terminal\n",
    "        param: agent failed or not\n",
    "        \"\"\"\n",
    "        next_state = np.append(self.currt_state[1:,:,:], o_next, axis=0)\n",
    "        self.replay_memory.append((self.currt_state, action, reward, next_state, terminal))\n",
    "        if len(self.replay_memory) > self.mem_size:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        if not terminal:\n",
    "            self.currt_state = next_state\n",
    "        else:\n",
    "            self.currt_state = self.empty_state\n",
    "\n",
    "        return\n",
    "    \n",
    "    def train_batch(self):\n",
    "        #train model one batch\n",
    "        minibatch = random.sample(self.replay_memory, self.batch_size)\n",
    "\n",
    "        state_batch = np.array([data[0] for data in minibatch])\n",
    "        action_batch = np.array([data[1] for data in minibatch])\n",
    "        reward_batch = np.array([data[2] for data in minibatch])\n",
    "        next_state_batch = np.array([data[3] for data in minibatch])\n",
    "\n",
    "        state_batch_var = Variable(torch.from_numpy(state_batch))\n",
    "        next_state_batch_var = Variable(torch.from_numpy(next_state_batch))\n",
    "\n",
    "        if self.is_cuda():\n",
    "            state_batch_var = state_batch_var.cuda()\n",
    "            next_state_batch_var = next_state_batch_var.cuda()\n",
    "\n",
    "        q_value_next = self.model(next_state_batch_var)\n",
    "        q_value = self.model(state_batch_var) \n",
    "\n",
    "        y_batch = reward_batch.astype(np._float32)\n",
    "        max_q, _ = torch.max(q_value_next, dim=1)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            terminal = minibatch[i][4]\n",
    "            if not terminal:\n",
    "                y_batch[i] += self.gamma*max_q.data[i][0]\n",
    "        \n",
    "        y_batch = Variable(torch.from_numpy(y_batch))\n",
    "        action_batch_var = Variable(torch.from_numpy(action_batch))\n",
    "        if self.is_cuda:\n",
    "            y_batch = y_batch.cuda()\n",
    "        q_value = torch.sum(torch.mul(action_batch_var, q_value), dim=1)\n",
    "\n",
    "        loss = self.ceriterion(q_value, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        #return random action\n",
    "        action = np.zeros(self.actions, dtype=np.float32)\n",
    "        action_index = 0 if random.random()<0.8 else 1\n",
    "        action[action_index] = 1\n",
    "        return action\n",
    "\n",
    "    def get_optim_action(self):\n",
    "        state_var = Variable(torch.from_numpy(self.currt_state))\n",
    "        if self.is_cuda:\n",
    "            state_var.cuda()\n",
    "        q_value = self.model(state_var)\n",
    "        _, action_index = torch.max(q_value, dim=1)\n",
    "        action_index = action_index.data[0]\n",
    "        action = np.zeros(self.actions, dtype=np.float32)\n",
    "        action[action_index] = 1\n",
    "        return action \n",
    "    \n",
    "    def init_optimizer(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.currt_state = self.empty_state\n",
    "\n",
    "    def increase_step(self):\n",
    "        self.time_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG(object):\n",
    "    is_training = True\n",
    "    is_cuda = True\n",
    "    lr = 0.01\n",
    "    mem_size = 5000\n",
    "    actions = 2\n",
    "    batch_size = 32\n",
    "    time_step = 0\n",
    "    init_e = 1\n",
    "    final_e = 0.1\n",
    "    gamma = 0.9\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dqn model\n",
    "dqn = DQNBrain(cfg)\n",
    "flappyBird = game.GameState()\n",
    "\n",
    "action = [1, 0]\n",
    "o, r, terminal = flappyBird.frame_step(action)\n",
    "o = preprocess(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init replay memory by random actions\n",
    "for i in range(observations):\n",
    "    action = dqn.get_random_action()\n",
    "    o, r, terminal = flappyBird.frame_step(action)\n",
    "    o = preprocess(o)\n",
    "    dqn.store_transition(o, action, r, terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
