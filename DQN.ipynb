{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True\n",
    "lr=0.001\n",
    "actions=2\n",
    "load_weight=False\n",
    "gamma=0.99\n",
    "batch_size=32\n",
    "mem_size=5000\n",
    "epsilon=0.9\n",
    "initial_epsilon=1.\n",
    "final_epsilon=0.1\n",
    "observation=100\n",
    "exploration=50000\n",
    "max_episode=100000\n",
    "save_checkpoint_freq = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.map_size = (64, 10, 10)\n",
    "        self.fc1 = nn.Linear(self.map_size[0]*self.map_size[1]*self.map_size[2], 256)\n",
    "        self.fc2 = nn.Linear(256, actions)\n",
    "\n",
    "    def foward(self, x):\n",
    "        #foward procedure to get MSE loss\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_frame = np.zeros((80, 80), dtype=np.float32)\n",
    "empty_state = np.stack((empty_frame, empty_frame, empty_frame, empty_frame), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "currt_state = empty_state\n",
    "replay_memory = deque()\n",
    "flappyBird = game.GameState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_randomly():\n",
    "    #random action\n",
    "    action = np.zeros(actions, dtype=np.float32)\n",
    "    action_index = 0 if random.random() < 0.8 else 1\n",
    "    action[action_index] = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_optim(model, currt_state):\n",
    "    #model.cuda()\n",
    "    state_var = Variable(torch.from_numpy(currt_state), volatile=True).unsqueeze(0)\n",
    "    #state_var.cuda()\n",
    "    q_value = model.foward(state_var)\n",
    "    _, action_index = torch.max(q_value, dim=1)\n",
    "    #print(action_index.data[0])\n",
    "    action_index = action_index.data[0]#[0][0]\n",
    "    action = np.zeros(actions, dtype=np.float32)\n",
    "    action[action_index] = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(model, currt_state):\n",
    "    if is_training and random.random() <= epsilon:\n",
    "        return get_action_randomly()\n",
    "    return get_action_optim(model, currt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transition(replay_memory, currt_state, param_dict):\n",
    "    o_next = param_dict[\"observation\"]\n",
    "    action = param_dict[\"action\"]\n",
    "    reward = param_dict[\"reward\"]\n",
    "    terminal = param_dict[\"terminal\"]\n",
    "    \n",
    "    #next_state = np.append(currt_state[1:,:,:], np.reshape(o_next, (1, o_next.shape[0], o_next.shape[1] )), axis=0)\n",
    "    next_state = np.append(currt_state[1:,:,:], o_next, axis=0)\n",
    "    #print(np.shape(next_state))\n",
    "    replay_memory.append((currt_state, action, reward, next_state, terminal))\n",
    "    if len(replay_memory) > mem_size:\n",
    "        replay_memory.popleft()\n",
    "    if not terminal:\n",
    "        currt_state = next_state\n",
    "    else:\n",
    "        currt_state = empty_state\n",
    "    return currt_state, replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert images to 80*80 gray images\n",
    "def preprocess(observation):\n",
    "    img = cv2.resize(observation, (80, 80))\n",
    "    observation = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(1,80,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "ceriterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(observation):\n",
    "    action = get_action_randomly()\n",
    "    o, r, terminal = flappyBird.frame_step(action)\n",
    "    o = preprocess(o)\n",
    "    param_dict = {\n",
    "            \"observation\":o,\n",
    "            \"action\":action,\n",
    "            \"reward\":r,\n",
    "            \"terminal\":terminal\n",
    "    }\n",
    "    currt_state, replay_memory = save_transition(replay_memory, currt_state, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(max_episode):\n",
    "    time_step = 0\n",
    "    total_reward = 0.\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        action = get_action(model, currt_state)\n",
    "        o_next, r, terminal = flappyBird.frame_step(action)\n",
    "        total_reward = total_reward*(gamma**time_step) + r\n",
    "        o_next = preprocess(o_next)\n",
    "        param_dict = {\n",
    "            \"observation\":o_next,\n",
    "            \"action\":action,\n",
    "            \"reward\":r,\n",
    "            \"terminal\":terminal\n",
    "        }\n",
    "        currt_state, replay_memory = save_transition(replay_memory, currt_state, param_dict)\n",
    "        time_step += 1\n",
    "        \n",
    "        minibatch = random.sample(replay_memory, batch_size)\n",
    "        \n",
    "        state_batch = np.array([data[0] for data in minibatch])\n",
    "        action_batch = np.array([data[1] for data in minibatch])\n",
    "        reward_batch = np.array([data[2] for data in minibatch])\n",
    "        next_state_batch = np.array([data[3] for data in minibatch])\n",
    "        #print(np.shape(next_state_batch))\n",
    "        \n",
    "        state_batch_var = Variable(torch.from_numpy(state_batch))\n",
    "        next_state_batch_var = Variable(torch.from_numpy(next_state_batch), volatile=True)\n",
    "        \n",
    "        #state_batch_var = state_batch_var.cuda()\n",
    "        #next_state_batch_var = next_state_batch_var.cuda()\n",
    "        #print(next_state_batch_var)\n",
    "        \n",
    "        q_value_next = model.foward(next_state_batch_var)\n",
    "        q_value = model.foward(state_batch_var)\n",
    "        \n",
    "        y = reward_batch.astype(np.float32)\n",
    "        max_q, _ = torch.max(q_value_next, dim=1)\n",
    "        #print(max_q)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if not minibatch[i][4]:\n",
    "                y[i] += gamma*max_q.data[i]\n",
    "                \n",
    "        y = Variable(torch.from_numpy(y))\n",
    "        action_batch_var = Variable(torch.from_numpy(action_batch))\n",
    "        \n",
    "        #y = y.cuda()\n",
    "        #action_batch_var = action_batch_var.cuda()\n",
    "        q_value = torch.sum(torch.mul(action_batch_var, q_value), dim=1)\n",
    "        \n",
    "        loss = ceriterion(q_value, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "    if epsilon > final_epsilon:\n",
    "        delta = (initial_epsilon - final_epsilon)/exploration\n",
    "        epsilon -= delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
